multi_node_cpu_cluster:
  cluster_name: "multi_node_cpu_cluster"
  spark_version: "13.3.x-cpu-ml-scala2.12"  # Use the latest stable CPU runtime
  node_type_id: "i3.xlarge"  # Example AWS node type (adjust based on cloud provider)
  driver_node_type_id: "i3.xlarge"  # Same as worker node or another type as needed
  autoscale:
    min_workers: 2
    max_workers: 10
  enable_elastic_disk: true
  aws_attributes:
    availability: "ON_DEMAND"  # Choose on-demand or spot instances for AWS

environments:
    defualt:
        strict_path_adjustment_policy: true
        workflows:
            - name: "pipeline1"
              job_cluster:
                - job_cluster_key: multi_node_cluster
                  new_cluster: *multi_node_cpu_cluster
              tasks:
                - task_key: "pipelin1"
                  python_wheel_task:
                    package_name: "premji_invest"
                    entry_point: "pipeline1"
                  existing_cluster_id: "your-cluster-id"
                  libraries:
                    - whl: "wheel_file"
                  email_notifications:
                  on_start:
                     - "email"
                  on_success:
                     - "email"
                  on_failure:
                     - "email"
                  timeout_seconds: 3600
                  max_retries: 2
                  min_retry_interval_millis: 300000
                  retry_on_timeout: true
                  schedule:
                    quartz_cron_expression: "0 30 13 * * ?"
                    timezone_id: "Asia/Kolkata"
                    pause_status: "UNPAUSED"
